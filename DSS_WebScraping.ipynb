{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup, element\n",
    "import re\n",
    "import openpyxl\n",
    "import pickle\n",
    "import json\n",
    "import geopy\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# a few handy url generator funcitons\n",
    "base_url = lambda start_num=1: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Search;;Start={start_num}\".format(start_num=start_num)\n",
    "loc_url = lambda loc_id: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Details;ID={loc_id}\".format(loc_id=loc_id)\n",
    "insp_url = lambda inspection_id, loc_id: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Inspection;Inspection={inspection_id};ID={loc_id}\".format(inspection_id=inspection_id, loc_id=loc_id)\n",
    "\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',  # Force certificate check.\n",
    "    ca_certs=certifi.where(),  # Path to the Certifi bundle.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1609cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I couldn't get the google maps api to work. I created an api_key under my personal google account but it didn't seem to work\n",
    "### I believe this was needed for some spatial analysis this project needed so not sure it's needed.\n",
    "\n",
    "# Define geolocator from GooglemapsV3 api :: no key required :: output projection EPSG:3857 Spherical Mercator (Web Mercator)\n",
    "#geolocator = geopy.geocoders.GoogleV3(api_key='AIzaSyDDWgZjAuas0i7U2BSpvGR8gn8KPMItLcE')\n",
    "#domain='maps.googleapis.com', \n",
    "\n",
    "def get_page(url):\n",
    "    '''Get all of our page data in a consistent fashion'''\n",
    "\n",
    "    r = http.request('GET', url)\n",
    "    # lxml is much better than stock python parser\n",
    "    return BeautifulSoup(r.data, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(tag):\n",
    "    '''a lot of time we will need to extract a key from a tag'''\n",
    "    return tag.get_text().strip().strip(':').lower().replace(' ', '_').encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a657580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_ids(start_num=1):\n",
    "    '''Location id Generator function.\n",
    "    This will automatically handeling paging of main id lookup, but you can skip ahead by passing in a number representing the ids place in global list, 1 based index'''\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        print ('Fetching some location ids')\n",
    "        soup = get_page(base_url(start_num))\n",
    "\n",
    "        num_locs = int(re.search('\\t(\\d{1,9}) records', soup.find_all('table')[1].find_all('td')[1].text).group(1))\n",
    "\n",
    "        ids = ([int(re.search(';ID=(\\d{1,9});', a['href']).group(1)) for a in soup.find_all('table')[3].find_all('a')])\n",
    "\n",
    "        for loc_id in ids:\n",
    "            start_num += 1\n",
    "            yield loc_id\n",
    "\n",
    "        if start_num >= num_locs:\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d918285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_loc(loc_id):\n",
    "    '''Fetch detailed info for a single location based on id'''\n",
    "\n",
    "    print(\"Fetching info for location id =\", loc_id)\n",
    "    \n",
    "    soup = get_page(loc_url(loc_id))\n",
    "\n",
    "    location_info = {\n",
    "        '_type': 'location_info',\n",
    "        'id': loc_id\n",
    "    }\n",
    "\n",
    "    # big breakdowns go by tables\n",
    "    basic_info, additional_info, inspection_info = soup.find_all('table')[:3]\n",
    "\n",
    "    # first table has a bunch of data in fairly unstructured format\n",
    "    name_and_address, city_zip, phone_number = basic_info.find_all('tr')\n",
    "\n",
    "    location_info.update({\n",
    "        'phone_number': phone_number.get_text().strip()\n",
    "    })\n",
    "\n",
    "    parsed_name_address = [line.strip() for line in name_and_address.get_text().split('\\n') if line.strip()]\n",
    "    location_info.update({\n",
    "        'name': parsed_name_address[0]\n",
    "    })\n",
    "    \n",
    "### Commented out Google API geolocator since API key didn't seem to work    \n",
    "    # Get address for geolocator with city, state and without \\n\n",
    "    #gcode_address = city_zip.get_text().strip()#' '.join([' '.join(parsed_name_address[1:]), city_zip.get_text().strip()])\n",
    "\n",
    "### Address needs to have name stripped from it.    \n",
    "    location_info.update({'address': name_and_address.get_text().strip()})\n",
    "\n",
    "    # there are a lot of additional info that follows the general format of <td>key</td><td>value</td>\n",
    "    # but some need some extra parsing\n",
    "    extra_parsing = {\n",
    "        'ages': lambda ages: ages.replace('\\t', '').replace('\\n', ''),\n",
    "        'inspector': lambda inspector_info: [line.strip() for line in inspector_info.split('\\n') if line.strip()]\n",
    "    }\n",
    "    for row in additional_info.find_all('tr')[:-1]:\n",
    "        key = get_key(row.find_all('td')[0])\n",
    "        val = row.find_all('td')[1].get_text().strip()\n",
    "        if key in extra_parsing:\n",
    "            val = extra_parsing[key](val)\n",
    "\n",
    "        location_info.update({key: val})\n",
    "\n",
    "    if 'inspector' in location_info:\n",
    "        location_info.update({\n",
    "            'inspector_name': location_info['inspector'][0],\n",
    "            'inspector_phone': location_info['inspector'][1]\n",
    "        })\n",
    "        del location_info['inspector']\n",
    "\n",
    "    if inspection_info.table:\n",
    "        inspection_ids = [int(re.search(';Inspection=(\\d{1,6});', tag.a['href']).group(1)) for tag in inspection_info.table.find_all('tr')[1:]]\n",
    "        location_info['inspections'] = [parse_inspection(insp_id, loc_id) for insp_id in inspection_ids]\n",
    "\n",
    "    else:\n",
    "        location_info['inspections'] = []\n",
    "    \n",
    "    return location_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20805a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inspection(insp_id, loc_id):\n",
    "    '''To get inspection data, you need to give the site both the inspection id and location id'''\n",
    "\n",
    "    #print (\" Fetching info for inspection id =\", insp_id)\n",
    "\n",
    "    soup = get_page(insp_url(insp_id, loc_id))\n",
    "\n",
    "    inspection_info = {\n",
    "        '_type': 'inspection_info',\n",
    "        'id': insp_id,\n",
    "        'loc_id': loc_id\n",
    "    }\n",
    "\n",
    "    # there is some redundant info about location, then some relevant stuff\n",
    "    date, complaint = soup.find('div', id='main_content').find_all('p')[3:5]\n",
    "    inspection_info.update({\n",
    "        'date': date.get_text().split('\\n')[5].strip(),\n",
    "        'complaint': complaint.get_text().split('\\n')[3].strip()\n",
    "    })\n",
    "    \n",
    "   ## pull in areas_reviewed    \n",
    "    tablenum = len(soup.find_all(\"table\"))\n",
    "\n",
    "    if tablenum == 1:\n",
    "        col = soup.find('dt')\n",
    "        col = col.get_text().split('\\n')[1].strip().strip(':')\n",
    "        tab = soup.find('table')\n",
    "        if col == 'Areas Reviewed':\n",
    "            inspection_info.update({\n",
    "                'areas_reviewed' : parse_areas_reviewed(tab)\n",
    "            })\n",
    "        if col == 'Comments':\n",
    "            inspection_info.update({\n",
    "                'comments' : tab.get_text().strip().replace('\\r', '\\n')\n",
    "            })\n",
    "    if tablenum == 2:\n",
    "        col1 = soup.find_all('dt')[0]\n",
    "        col1 = col1.get_text().strip().replace('\\r', '\\n')\n",
    "        col2 = soup.find_all('dt')[1]\n",
    "        col2 = col2.get_text().strip().replace('\\r', '\\n')\n",
    "        col1,col2 = soup.find_all('table')[:2]\n",
    "        if col1 == 'Areas Reviewed':\n",
    "            inspection_info.update({\n",
    "                'areas_reviewed' : parse_areas_reviewed(col1)\n",
    "            })\n",
    "        elif col1 == 'Comments':\n",
    "            inspection_info.update({\n",
    "                'comments' : col1.get_text().strip().replace('\\r', '\\n')\n",
    "            })\n",
    "        else: inspection_info.update({\n",
    "                'violations' : 'To update'\n",
    "            })\n",
    "\n",
    "        if col2 == 'Areas Reviewed':\n",
    "            inspection_info.update({\n",
    "                'areas_reviewed' : parse_areas_reviewed(col2)\n",
    "            })\n",
    "        elif col2 == 'Comments':\n",
    "            inspection_info.update({\n",
    "                'comments' : col2.get_text().strip().replace('\\r', '\\n')\n",
    "            })\n",
    "        else: inspection_info.update({\n",
    "                'violations' : 'To update'\n",
    "            })    \n",
    "\n",
    "    if tablenum == 3:\n",
    "        areas_reviewed,comments,violations=soup.find_all('table')[:3]\n",
    "        inspection_info.update({\n",
    "            'areas_reviewed': parse_areas_reviewed(areas_reviewed),\n",
    "            'comments': comments.get_text().strip().replace('\\r', '\\n')\n",
    "            #'violations': parse_volations(violations)\n",
    "        })\n",
    "\n",
    "\n",
    "  \n",
    "    return inspection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need a lot of specialized parsers\n",
    "def parse_violations(violations):\n",
    "\n",
    "    parsers = {\n",
    "        'standard_#': lambda val: val.strip(),\n",
    "        'description': lambda val: val.strip().replace('\\r', '\\n'),\n",
    "        'complaint_related': lambda val: val.strip(),\n",
    "        'action_to_be_taken': lambda val: val.strip().replace('\\r', '\\n')\n",
    "    }\n",
    "    line_num = 0\n",
    "    violation_lines = violations.find_all('tr')\n",
    "    violations_info = []\n",
    "    violation_info = {}\n",
    "\n",
    "    while line_num < len(violation_lines):\n",
    "\n",
    "        if violation_lines[line_num].td is None:\n",
    "            # there seems to be blank lines after 'complain_related' that don't have <td>s\n",
    "            pass\n",
    "\n",
    "        elif violation_lines[line_num].hr:\n",
    "            violations_info.append(violation_info)\n",
    "            violation_info = {}\n",
    "\n",
    "        else:\n",
    "            raw_key, val = violation_lines[line_num].get_text().split(':', 1)\n",
    "            key = raw_key.strip().strip(':').lower().replace(' ', '_').encode('ascii', 'ignore')\n",
    "            violation_info[key] = parsers[key](val)\n",
    "\n",
    "        line_num += 1\n",
    "\n",
    "    return violations_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c103ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_areas_reviewed(areas_reviewed):\n",
    "    if areas_reviewed.br:\n",
    "        return [areas_reviewed.br.previousSibling.strip()] + [foo.nextSibling.strip() for foo in areas_reviewed.find_all('br') if isinstance(foo.nextSibling, element.NavigableString)]\n",
    "\n",
    "    return [areas_reviewed.td.text.strip()]\n",
    "\n",
    "    parsers = {\n",
    "        'areas_reviewed': parse_areas_reviewed,\n",
    "        'technical_assistance': lambda technical_assistance: technical_assistance.get_text().strip(),\n",
    "        'comments': lambda comments: comments.get_text().strip().replace('\\r', '\\n'),\n",
    "        'violations': parse_violations\n",
    "    }\n",
    "\n",
    "    # also have a variable number of tables, id'd by <dt>s\n",
    "    table_ids = [get_key(tag) for tag in soup.find_all('dt')]\n",
    "\n",
    "    for key, tag in zip(table_ids, soup.find_all('table')[:len(table_ids)]):\n",
    "        inspection_info[key] = parsers[key](tag)\n",
    "\n",
    "    return inspection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a0451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get all location IDs\n",
    "ids = list(get_loc_ids())\n",
    "#df_ids = pd.DataFrame(ids,columns=[\"LocationIDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2684207",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing top N location IDs to view data\n",
    "top_n_idx = np.argsort(ids)[-10:]\n",
    "top_n_values = [ids[i] for i in top_n_idx]\n",
    "top_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d775d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## del df\n",
    "\n",
    "## Create dataframe to append data to\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_id = ids ## All facilities\n",
    "    #top_n_values ## Sample of facilities \n",
    "for i in loc_id:\n",
    "    parse_loc(i)\n",
    "    df = df.append(parse_loc(i), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05625232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dss.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
