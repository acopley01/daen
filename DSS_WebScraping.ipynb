{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab84303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup, element\n",
    "import re\n",
    "import openpyxl\n",
    "import pickle\n",
    "import json\n",
    "import geopy\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# a few handy url generator funcitons\n",
    "base_url = lambda start_num=1: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Search;;Start={start_num}\".format(start_num=start_num)\n",
    "loc_url = lambda loc_id: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Details;ID={loc_id}\".format(loc_id=loc_id)\n",
    "insp_url = lambda inspection_id, loc_id: \"https://www.dss.virginia.gov/facility/search/alf.cgi?rm=Inspection;Inspection={inspection_id};ID={loc_id}\".format(inspection_id=inspection_id, loc_id=loc_id)\n",
    "\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',  # Force certificate check.\n",
    "    ca_certs=certifi.where(),  # Path to the Certifi bundle.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cead4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I couldn't get the google maps api to work. I created an api_key under my personal google account but it didn't seem to work\n",
    "### I believe this was needed for some spatial analysis this project needed so not sure it's needed.\n",
    "\n",
    "# Define geolocator from GooglemapsV3 api :: no key required :: output projection EPSG:3857 Spherical Mercator (Web Mercator)\n",
    "#geolocator = geopy.geocoders.GoogleV3(api_key='AIzaSyDDWgZjAuas0i7U2BSpvGR8gn8KPMItLcE')\n",
    "#domain='maps.googleapis.com', \n",
    "\n",
    "def get_page(url):\n",
    "    '''Get all of our page data in a consistent fashion'''\n",
    "\n",
    "    r = http.request('GET', url)\n",
    "    # lxml is much better than stock python parser\n",
    "    return BeautifulSoup(r.data, 'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa943563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(tag):\n",
    "    '''a lot of time we will need to extract a key from a tag'''\n",
    "    return tag.get_text().strip().strip(':').lower().replace(' ', '_').encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e37b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_ids(start_num=1):\n",
    "    '''Location id Generator function.\n",
    "    This will automatically handeling paging of main id lookup, but you can skip ahead by passing in a number representing the ids place in global list, 1 based index'''\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        #print ('Fetching some location ids')\n",
    "        soup = get_page(base_url(start_num))\n",
    "\n",
    "        num_locs = int(re.search('\\t(\\d{1,9}) records', soup.find_all('table')[1].find_all('td')[1].text).group(1))\n",
    "\n",
    "        ids = ([int(re.search(';ID=(\\d{1,9});', a['href']).group(1)) for a in soup.find_all('table')[3].find_all('a')])\n",
    "\n",
    "        for loc_id in ids:\n",
    "            start_num += 1\n",
    "            yield loc_id\n",
    "\n",
    "        if start_num >= num_locs:\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e5cc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_loc(loc_id):\n",
    "    '''Fetch detailed info for a single location based on id'''\n",
    "\n",
    "    #print(\"Fetching info for location id =\", loc_id)\n",
    "    \n",
    "    soup = get_page(loc_url(loc_id))\n",
    "\n",
    "    location_info = {\n",
    "        '_type': 'location_info',\n",
    "        'id': loc_id\n",
    "    }\n",
    "\n",
    "    # big breakdowns go by tables\n",
    "    basic_info, additional_info, inspection_info = soup.find_all('table')[:3]\n",
    "\n",
    "    # first table has a bunch of data in fairly unstructured format\n",
    "    name_and_address, city_zip, phone_number = basic_info.find_all('tr')\n",
    "\n",
    "    location_info.update({\n",
    "        'phone_number': phone_number.get_text().strip()\n",
    "    })\n",
    "\n",
    "    parsed_name_address = [line.strip() for line in name_and_address.get_text().split('\\n') if line.strip()]\n",
    "    location_info.update({\n",
    "        'name': parsed_name_address[0]\n",
    "    })\n",
    "    \n",
    "### Commented out Google API geolocator since API key didn't seem to work    \n",
    "    # Get address for geolocator with city, state and without \\n\n",
    "    #gcode_address = city_zip.get_text().strip()#' '.join([' '.join(parsed_name_address[1:]), city_zip.get_text().strip()])\n",
    "\n",
    "### Address needs to have name stripped from it.    \n",
    "    location_info.update({'address': name_and_address.get_text().strip()})\n",
    "\n",
    "    # there are a lot of additional info that follows the general format of <td>key</td><td>value</td>\n",
    "    # but some need some extra parsing\n",
    "    extra_parsing = {\n",
    "        'ages': lambda ages: ages.replace('\\t', '').replace('\\n', ''),\n",
    "        'inspector': lambda inspector_info: [line.strip() for line in inspector_info.split('\\n') if line.strip()]\n",
    "    }\n",
    "    for row in additional_info.find_all('tr')[:-1]:\n",
    "        key = get_key(row.find_all('td')[0])\n",
    "        val = row.find_all('td')[1].get_text().strip()\n",
    "        if key in extra_parsing:\n",
    "            val = extra_parsing[key](val)\n",
    "\n",
    "        location_info.update({key: val})\n",
    "\n",
    "    if 'inspector' in location_info:\n",
    "        location_info.update({\n",
    "            'inspector_name': location_info['inspector'][0],\n",
    "            'inspector_phone': location_info['inspector'][1]\n",
    "        })\n",
    "        del location_info['inspector']\n",
    "\n",
    "    if inspection_info.table:\n",
    "        inspection_ids = [int(re.search(';Inspection=(\\d{1,6});', tag.a['href']).group(1)) for tag in inspection_info.table.find_all('tr')[1:]]\n",
    "        location_info['inspections'] = [parse_inspection(insp_id, loc_id) for insp_id in inspection_ids]\n",
    "\n",
    "    else:\n",
    "        location_info['inspections'] = []\n",
    "    \n",
    "    return location_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02cf1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inspection(insp_id, loc_id):\n",
    "    '''To get inspection data, you need to give the site both the inspection id and location id'''\n",
    "\n",
    "    #print (\" Fetching info for inspection id =\", insp_id)\n",
    "\n",
    "    soup = get_page(insp_url(insp_id, loc_id))\n",
    "\n",
    "    inspection_info = {\n",
    "        '_type': 'inspection_info',\n",
    "        'id': insp_id,\n",
    "        'loc_id': loc_id\n",
    "    }\n",
    "\n",
    "    # there is some redundant info about location, then some relevant stuff\n",
    "    date, complaint = soup.find('div', id='main_content').find_all('p')[3:5]\n",
    "    inspection_info.update({\n",
    "        'date': date.get_text().split('\\n')[5].strip(),\n",
    "        'complaint': complaint.get_text().split('\\n')[3].strip()\n",
    "    })\n",
    "    \n",
    "   ## pull in areas_reviewed    \n",
    "    areas_reviewed,comments=soup.find_all('table')[:2]\n",
    "    inspection_info.update({\n",
    "        'areas_reviewed': parse_areas_reviewed(areas_reviewed),\n",
    "        'comments': parse_areas_reviewed(comments)\n",
    "    })\n",
    "    \n",
    "\n",
    "  \n",
    "    return inspection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1a7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need a lot of specialized parsers\n",
    "def parse_violations(violations):\n",
    "\n",
    "    parsers = {\n",
    "        'standard_#': lambda val: val.strip(),\n",
    "        'description': lambda val: val.strip().replace('\\r', '\\n'),\n",
    "        'complaint_related': lambda val: val.strip(),\n",
    "        'action_to_be_taken': lambda val: val.strip().replace('\\r', '\\n')\n",
    "    }\n",
    "    line_num = 0\n",
    "    violation_lines = violations.find_all('tr')\n",
    "    violations_info = []\n",
    "    violation_info = {}\n",
    "\n",
    "    while line_num < len(violation_lines):\n",
    "\n",
    "        if violation_lines[line_num].td is None:\n",
    "            # there seems to be blank lines after 'complain_related' that don't have <td>s\n",
    "            pass\n",
    "\n",
    "        elif violation_lines[line_num].hr:\n",
    "            violations_info.append(violation_info)\n",
    "            violation_info = {}\n",
    "\n",
    "        else:\n",
    "            raw_key, val = violation_lines[line_num].get_text().split(':', 1)\n",
    "            key = raw_key.strip().strip(':').lower().replace(' ', '_').encode('ascii', 'ignore')\n",
    "            violation_info[key] = parsers[key](val)\n",
    "\n",
    "        line_num += 1\n",
    "\n",
    "    return violations_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab99b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_areas_reviewed(areas_reviewed):\n",
    "    if areas_reviewed.br:\n",
    "        return [areas_reviewed.br.previousSibling.strip()] + [foo.nextSibling.strip() for foo in areas_reviewed.find_all('br') if isinstance(foo.nextSibling, element.NavigableString)]\n",
    "\n",
    "    return [areas_reviewed.td.text.strip()]\n",
    "\n",
    "    parsers = {\n",
    "        'areas_reviewed': parse_areas_reviewed,\n",
    "        'technical_assistance': lambda technical_assistance: technical_assistance.get_text().strip(),\n",
    "        'comments': lambda comments: comments.get_text().strip().replace('\\r', '\\n'),\n",
    "        'violations': parse_violations\n",
    "    }\n",
    "\n",
    "    # also have a variable number of tables, id'd by <dt>s\n",
    "    table_ids = [get_key(tag) for tag in soup.find_all('dt')]\n",
    "\n",
    "    for key, tag in zip(table_ids, soup.find_all('table')[:len(table_ids)]):\n",
    "        inspection_info[key] = parsers[key](tag)\n",
    "\n",
    "    return inspection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed84c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get all location IDs\n",
    "ids = list(get_loc_ids())\n",
    "#df_ids = pd.DataFrame(ids,columns=[\"LocationIDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5615a276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47824, 47825, 47826, 47827, 47828, 47869, 47870, 47883, 47918, 48088]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing top N location IDs to view data\n",
    "top_n_idx = np.argsort(ids)[-10:]\n",
    "top_n_values = [ids[i] for i in top_n_idx]\n",
    "top_n_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b7cb1e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## del df\n",
    "\n",
    "## Create dataframe to append data to\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57e650fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching info for location id = 8364\n",
      " Fetching info for inspection id = 29026\n",
      " Fetching info for inspection id = 27827\n",
      " Fetching info for inspection id = 26908\n",
      " Fetching info for inspection id = 26257\n",
      " Fetching info for inspection id = 24888\n",
      "Fetching info for location id = 8364\n",
      " Fetching info for inspection id = 29026\n",
      " Fetching info for inspection id = 27827\n",
      " Fetching info for inspection id = 26908\n",
      " Fetching info for inspection id = 26257\n",
      " Fetching info for inspection id = 24888\n",
      "Fetching info for location id = 21384\n",
      " Fetching info for inspection id = 30175\n",
      " Fetching info for inspection id = 30029\n",
      " Fetching info for inspection id = 28403\n",
      " Fetching info for inspection id = 27808\n",
      " Fetching info for inspection id = 27422\n",
      " Fetching info for inspection id = 27137\n",
      " Fetching info for inspection id = 26678\n",
      " Fetching info for inspection id = 25792\n",
      " Fetching info for inspection id = 24587\n",
      "Fetching info for location id = 21384\n",
      " Fetching info for inspection id = 30175\n",
      " Fetching info for inspection id = 30029\n",
      " Fetching info for inspection id = 28403\n",
      " Fetching info for inspection id = 27808\n",
      " Fetching info for inspection id = 27422\n",
      " Fetching info for inspection id = 27137\n",
      " Fetching info for inspection id = 26678\n",
      " Fetching info for inspection id = 25792\n",
      " Fetching info for inspection id = 24587\n",
      "Fetching info for location id = 24143\n",
      " Fetching info for inspection id = 30504\n",
      " Fetching info for inspection id = 29090\n",
      " Fetching info for inspection id = 28042\n",
      " Fetching info for inspection id = 27179\n",
      " Fetching info for inspection id = 26391\n",
      " Fetching info for inspection id = 25977\n",
      " Fetching info for inspection id = 25319\n",
      "Fetching info for location id = 24143\n",
      " Fetching info for inspection id = 30504\n",
      " Fetching info for inspection id = 29090\n",
      " Fetching info for inspection id = 28042\n",
      " Fetching info for inspection id = 27179\n",
      " Fetching info for inspection id = 26391\n",
      " Fetching info for inspection id = 25977\n",
      " Fetching info for inspection id = 25319\n",
      "Fetching info for location id = 46839\n",
      " Fetching info for inspection id = 30635\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-c703b62c953e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#top_n_values ## Sample of facilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloc_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mparse_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1a861e8d281e>\u001b[0m in \u001b[0;36mparse_loc\u001b[1;34m(loc_id)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minspection_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0minspection_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';Inspection=(\\d{1,6});'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minspection_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mlocation_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inspections'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparse_inspection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minsp_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minsp_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minspection_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1a861e8d281e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minspection_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0minspection_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';Inspection=(\\d{1,6});'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minspection_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mlocation_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inspections'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparse_inspection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minsp_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minsp_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minspection_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-e5cb446bf14a>\u001b[0m in \u001b[0;36mparse_inspection\u001b[1;34m(insp_id, loc_id)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m    \u001b[1;31m## pull in areas_reviewed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mareas_reviewed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     inspection_info.update({\n\u001b[0;32m     24\u001b[0m         \u001b[1;34m'areas_reviewed'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparse_areas_reviewed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mareas_reviewed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "loc_id = ids ## All facilities\n",
    "    #top_n_values ## Sample of facilities \n",
    "for i in loc_id:\n",
    "    parse_loc(i)\n",
    "    df = df.append(parse_loc(i), ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9585c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df.to_csv('dss.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a4192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Used for testing specific locations/inspections\n",
    "soup = get_page(insp_url(30548,47918))\n",
    "#areas_reviewed = soup.find('div', id='main_content').find_all('b')[3:8]\n",
    "#areas_reviewed,comments,violations =soup.find_all('table')[:2]\n",
    "#areas_reviewed.find_all('br')\n",
    "#test = parse_areas_reviewed(areas_reviewed)\n",
    "#test2 = parse_areas_reviewed(comments)\n",
    "#comments\n",
    "#areas_reviewed = areas_reviewed.find_all('tr')\n",
    "#test = areas_reviewed.get_text().strip()\n",
    "#test = areas_reviewed.td.text.strip()\n",
    "vio = soup.find('div', id='main_content').find_all('a name')\n",
    "vio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_locs_csv(loc_array, file_name='dss_virginia.csv'):\n",
    "    field_names = [\n",
    "        'id',\n",
    "        'phone_number',\n",
    "        'business_hours',\n",
    "        'name',\n",
    "        'locality',\n",
    "        'inspector_phone',\n",
    "        'address',\n",
    "        'ages',\n",
    "        'administrator',\n",
    "        'longitude',\n",
    "        'facility_type',\n",
    "        'latitude',\n",
    "        'expiration_date',\n",
    "        'license_type',\n",
    "        'capacity',\n",
    "        'inspector_name',\n",
    "        'fips'\n",
    "    ]\n",
    "    import csv\n",
    "    with open(file_name, 'wb') as csvfile:\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=field_names, extrasaction='ignore')\n",
    "\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        for loc in loc_array:\n",
    "            csv_writer.writerows(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6438b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_locs_xslx(loc_array, file_name= \"dss_virginia.xlsx\"):\n",
    "    loc_field_order = ['id', 'name', 'facility_type', 'license_type', 'capacity', 'locality', 'ages', 'address', 'phone_number', 'fips', 'web_link']\n",
    "    wb = openpyxl.Workbook()\n",
    "\n",
    "    loc_ws = wb['Sheet']\n",
    "    loc_ws.title = 'Location Information'\n",
    "\n",
    "    for c, field in enumerate(loc_field_order):\n",
    "        loc_ws.cell(row=1, column=c + 1).value = field\n",
    "\n",
    "    for i, loc_info in enumerate(loc_array):\n",
    "        for c, field in enumerate(loc_field_order):\n",
    "            if field != 'web_link':\n",
    "                loc_ws.cell(row=i + 2, column=c + 1).value = loc_info.get(field, None)\n",
    "            else:\n",
    "                loc_ws.cell(row=i + 2, column=c + 1).value = loc_url(loc_info['id'])\n",
    "\n",
    "    wb.save(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
